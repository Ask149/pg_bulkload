pg_bulkload - High speed data loading utility

    Copyright(C) 2007-2008 Nippon Telegraph and Telephone Corporation

Introduction
------------

pg_bulkload provides high-speed data loading capability to PostgreSQL users.

When we load huge amount of data to a database, it is common situation that 
data set to be loaded is valid and consistent.   For example, dedicated tools 
are used to prepare such data, providing data validation in advance.   In such 
cases, we'd like to bypass any overheads within database system to load data as 
quickly as possible.  pg_bulkload is developed to help such situations.   
Therefore, it is not pg_bulkload's goal to provide detailed data validation.  
Rather, pg_bulkload asumes that loaded data set is validated by separate means.
If you're not in such situation, you should use COPY command in PostgreSQL.

Lineup
------

pg_bulkload provides two programs to users.

1.	pg_bulkload
  This program is used to load the data.   Internally, it invokes PostgreSQL's 
  user-defined function called pg_bulkload() and perform the loading.   
  pg_bulkload() function will be installed during pg_bulkload installation.

2.	postgresql script
  This is a wrapper command for pg_ctl, which starts and stops PostgreSQL 
  server.  postgresql script invokes pg_ctl internally.   postgresql script 
  provides very important pg_bulkload functionality, recovery.   For 
  performance, pg_bulkload bypasses some of PostgreSQL's internal functionality
  such as WAL.   Therefore, pg_bulkload needs to provide separate recovery 
  procedure before usual PostgreSQL's recovery is performed.   postgresql script
  provides this feature.


  If you use pg_bulkload, you must not invoke pg_ctl directly.   Instead, use 
  postgres script.


  In addition, the following user-defined function is also provided.

3.	pg_timestamp_in
  This user-defined function provides very fast loading of timestamp type data.
  For the speed, instead, the format of the timestamp data must satisfy the 
  following 19byte format.

    2007-01-01 12:34:56

	  - Year/Month/Day/Hour/Minute/Second, each must be represented using 
	    only digits (0x30 to 0x39, no month name, no AM/PM, etc.),
	  - Only a hyphen (0x2d) can be used (and must be used) to separate 
	    year, month and day,
	  - Single space (0x20) must appear between the day and the hour.
	  - Hour, minute and second must be separated using single colon (0x3a)
	    respectively.
	  - No additional spaces and tabs are allowed.




Installation
------------

1. Environment

pg_bulkload installation assumes the following;
- PostgreSQL must have been installed in advance,
- Source tree used to install the PostgreSQL above is available, and
- The database has been initialized using initdb.

2. Installation procedure

Installation sequence is shown below. Parmission for installed directories are 
given correctly.

$ cd <directory where postgresql-8.2or3.X.tar.gz is untared>/contrib/
$ tar zxvf pg_bulkload-2.3.X.tar.gz
$ cd pg_bulkload
$ make
$ make install
$ mkdir $PGDATA/pg_bulkload
$ postgresql start
$ psql -f $PGHOME/share/contrib/pg_bulkload.sql database_name
$ psql -f $PGHOME/share/contrib/pg_timestamp.sql database_name


Usage
-----
1. Edit control file "sample_csv.ctl" or "sample_bin.ctl" that includes
   settigs for data loading. You can specify table name, absolute path
   for input file, description of the input file, and so on.

2. Assume there is a directory $PGDATA/pg_bulkload, in that load status
   files are created.

3. Execute command with a control file as argument. Relative path is available 
   for the argument.

     $ pg_bulkload sample_csv.ctl 


Reminder
--------
* pg_bulkload
  If you use pg_bulkload, you must start and stop PostgreSQL using postgresql 
  script, which is installed together with pg_bulkload command.   Do not use 
  pg_ctl directly.
  
  If you use pg_bulkload, because it bypasses WAL, archive recovery by PITR is 
  not available.   If you'd like to use PITR, take a full backup of the 
  database after the loading by pg_bulkload.
  
  Only unique constraint and not null constraint are enforced during the 
  loading.   Other constraints such as referentiial integrity are not enforced.
  It is user's responsibility to provide valid data set.
  
  You must not remove the load status file (*.loadstatus) found in 
  $PGDATA/pg_bulkload directory.   This file is needed in pg_bulkload crash 
  recovery.
  
  Do not terminate pg_bulkload command using "kill -9".   If you did this, you 
  must invoke postgresql script to perform pg_bulkload recovery and restart 
  PostgreSQL to continue.
  
* pg_timestamp
  Timestamp value with timezone attribute is outside the scope of 
  pg_timestamp_in.   If you provide data in such format, you must use usual 
  PostgreSQL feature to read data.   In this case, you may need longer duration
  to load.  Although pg_timestamp_in provide much faster data loading for 
  timestamp data, it replaces usual PostgreSQL's internal function used to read
  timestamp data.   That is, use of pg_timestamp_in influences the data symtax 
  for PostgreSQL's SQL statements such as INSERT, COPY, and UPDATE.   To avoid 
  such influence, users have to use pg_timestamp_in only in the data loading, 
  and then set back pg_timestamp_in to the original function.


Control File
------------
* Common 
TABLE       : [<schema_name>.]table_name
INFILE      : Input data location(absolute path)
TYPE        : CSV or FIXED
MAX_ERR_CNT : The number of errors to stop loading. The default is 0, that is
              all of data is loaded at once in spite of occuring errors.
OFFSET      : The number of skip input rows. The default is 0.
LIMIT       : The number of rows to load. The default is 0, that is all of data 
              is loaded.

* CSV format
DELIMITER      : The single ASCII character that separates columns within each row
                 (line) of the file. The default is comma.
QUOTE          : Specifies the ASCII quotation character. 
                 The default is double-quotation.
ESCAPE         : Specifies the ASCII character that should appear before a QUOTE
                 data character value. The default is double-quotation.
NULL           : The string that represents a null value.  The default is a empty
                 value with no quotes.
FORCE_NOT_NULL : Process each specified column as though it were not a NULL value.
                 Multiple columns are available as needed.

* Fixed format
COL             : Column format of input file.
  COL TYPE(N)   : N bytes of TYPE. TYPE should be CHAR, VARCHAR, INTEGER or FLOAT.
                  CHAR and VARCHAR means the input column is text, and INTEGER and
                  FLOAT means it is binary.
  COL TYPE      : TYPE with default length. TYPE should be one of the following:
                    SMALLINT = INTGEGER(2)
                    INTEGER  = INTEGER(4)
                    BIGINT   = INTEGER(8)
                    FLOAT    = FLOAT(4)
                    DOUBLE   = FLOAT(8)
  COL TYPE(N+M) : N bytes, offset M bytes
  COL TYPE(N:M) : start at N bytes and end at M bytes
  COL N         : Same as COL CHAR(N), for backward compatibility.
PRESERVE_BLANKS : {YES|NO} YES regards following "COL N" as "COL CHAR(N)" and
                  NO as "COL VARCHAR(N)". Default is NO.
STRIDE          : Length of one row. Use if you want to truncate the end of row.
                  The default is whole of the row, which means the total of COLs.

Author
------
NTT Opensource Software Center
